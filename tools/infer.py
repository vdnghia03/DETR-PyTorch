# import torch
# import argparse
# import os
# import yaml
# import random
# from tqdm import tqdm
# from model.detr import DETR
# import numpy as np
# import cv2
# from dataset.food67 import Food67Dataset
# from torch.utils.data.dataloader import DataLoader

# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# if torch.backends.mps.is_available():
#     device = torch.device('mps')
#     print('Using mps')


# def get_iou(det, gt):
#     det_x1, det_y1, det_x2, det_y2 = det
#     gt_x1, gt_y1, gt_x2, gt_y2 = gt

#     x_left = max(det_x1, gt_x1)
#     y_top = max(det_y1, gt_y1)
#     x_right = min(det_x2, gt_x2)
#     y_bottom = min(det_y2, gt_y2)

#     if x_right < x_left or y_bottom < y_top:
#         return 0.0

#     area_intersection = (x_right - x_left) * (y_bottom - y_top)
#     det_area = (det_x2 - det_x1) * (det_y2 - det_y1)
#     gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)
#     area_union = float(det_area + gt_area - area_intersection + 1E-6)
#     iou = area_intersection / area_union
#     return iou


# def compute_map(det_boxes, gt_boxes, iou_threshold=0.5, method='area', difficult=None):
#     # det_boxes = [
#     #   {
#     #       'person' : [[x1, y1, x2, y2, score], ...],
#     #       'car' : [[x1, y1, x2, y2, score], ...]
#     #   }
#     #   {det_boxes_img_2},
#     #   ...
#     #   {det_boxes_img_N},
#     # ]
#     #
#     # gt_boxes = [
#     #   {
#     #       'person' : [[x1, y1, x2, y2], ...],
#     #       'car' : [[x1, y1, x2, y2], ...]
#     #   },
#     #   {gt_boxes_img_2},
#     #   ...
#     #   {gt_boxes_img_N},
#     # ]

#     gt_labels = {cls_key for im_gt in gt_boxes for cls_key in im_gt.keys()}
#     gt_labels = sorted(gt_labels)

#     all_aps = {}
#     # average precisions for ALL classes
#     aps = []
#     for idx, label in enumerate(gt_labels):
#         # Get detection predictions of this class
#         cls_dets = [
#             [im_idx, im_dets_label] for im_idx, im_dets in enumerate(det_boxes)
#             if label in im_dets for im_dets_label in im_dets[label]
#         ]

#         # cls_dets = [
#         #   (0, [x1_0, y1_0, x2_0, y2_0, score_0]),
#         #   ...
#         #   (0, [x1_M, y1_M, x2_M, y2_M, score_M]),
#         #   (1, [x1_0, y1_0, x2_0, y2_0, score_0]),
#         #   ...
#         #   (1, [x1_N, y1_N, x2_N, y2_N, score_N]),
#         #   ...
#         # ]

#         # Sort them by confidence score
#         cls_dets = sorted(cls_dets, key=lambda k: -k[1][-1])

#         # For tracking which gt boxes of this class have already been matched
#         gt_matched = [[False for _ in im_gts[label]] for im_gts in gt_boxes]
#         # Number of gt boxes for this class for recall calculation
#         num_gts = sum([len(im_gts[label]) for im_gts in gt_boxes])
#         num_difficults = sum([sum(difficults_label[label])
#                               for difficults_label in difficult])

#         tp = [0] * len(cls_dets)
#         fp = [0] * len(cls_dets)

#         # For each prediction
#         for det_idx, (im_idx, det_pred) in enumerate(cls_dets):
#             # Get gt boxes for this image and this label
#             im_gts = gt_boxes[im_idx][label]
#             im_gt_difficults = difficult[im_idx][label]

#             max_iou_found = -1
#             max_iou_gt_idx = -1

#             # Get best matching gt box
#             for gt_box_idx, gt_box in enumerate(im_gts):
#                 gt_box_iou = get_iou(det_pred[:-1], gt_box)
#                 if gt_box_iou > max_iou_found:
#                     max_iou_found = gt_box_iou
#                     max_iou_gt_idx = gt_box_idx
#             # TP only if iou >= threshold and this gt has not yet been matched
#             if max_iou_found >= iou_threshold:
#                 if not gt_matched[im_idx][max_iou_gt_idx]:
#                     # If tp then we set this gt box as matched
#                     gt_matched[im_idx][max_iou_gt_idx] = True
#                     tp[det_idx] = 1
#                 else:
#                     fp[det_idx] = 1
#             else:
#                 fp[det_idx] = 1

#         # Cumulative tp and fp
#         tp = np.cumsum(tp)
#         fp = np.cumsum(fp)

#         eps = np.finfo(np.float32).eps
#         # recalls = tp / np.maximum(num_gts, eps)
#         recalls = tp / np.maximum(num_gts - num_difficults, eps)
#         precisions = tp / np.maximum((tp + fp), eps)

#         if method == 'area':
#             recalls = np.concatenate(([0.0], recalls, [1.0]))
#             precisions = np.concatenate(([0.0], precisions, [0.0]))

#             # Replace precision values with recall r with maximum precision value
#             # of any recall value >= r
#             # This computes the precision envelope
#             for i in range(precisions.size - 1, 0, -1):
#                 precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])
#             # For computing area, get points where recall changes value
#             i = np.where(recalls[1:] != recalls[:-1])[0]
#             # Add the rectangular areas to get ap
#             ap = np.sum((recalls[i + 1] - recalls[i]) * precisions[i + 1])
#         elif method == 'interp':
#             ap = 0.0
#             for interp_pt in np.arange(0, 1 + 1E-3, 0.1):
#                 # Get precision values for recall values >= interp_pt
#                 prec_interp_pt = precisions[recalls >= interp_pt]

#                 # Get max of those precision values
#                 prec_interp_pt= prec_interp_pt.max() if prec_interp_pt.size>0.0 else 0.0
#                 ap += prec_interp_pt
#             ap = ap / 11.0
#         else:
#             raise ValueError('Method can only be area or interp')
#         if num_gts > 0:
#             aps.append(ap)
#             all_aps[label] = ap
#         else:
#             all_aps[label] = np.nan
#     # compute mAP at provided iou threshold
#     mean_ap = sum(aps) / len(aps)
#     return mean_ap, all_aps


# def load_model_and_dataset(args):
#     # Read the config file #
#     with open(args.config_path, 'r') as file:
#         try:
#             config = yaml.safe_load(file)
#         except yaml.YAMLError as exc:
#             print(exc)
#     print(config)
#     ########################

#     dataset_config = config['dataset_params']
#     model_config = config['model_params']
#     train_config = config['train_params']

#     voc = VOCDataset('test',
#                      im_sets=dataset_config['test_im_sets'],
#                      im_size=dataset_config['im_size'])
#     test_dataset = DataLoader(voc, batch_size=1, shuffle=False)

#     model = DETR(
#         config=model_config,
#         num_classes=dataset_config['num_classes'],
#         bg_class_idx=dataset_config['bg_class_idx']
#     )
#     model.to(device=torch.device(device))
#     model.eval()

#     assert os.path.exists(os.path.join(train_config['task_name'],
#                                        train_config['ckpt_name'])), \
#         "No checkpoint exists at {}".format(os.path.join(train_config['task_name'],
#                                                          train_config['ckpt_name']))
#     model.load_state_dict(torch.load(os.path.join(train_config['task_name'],
#                                                        train_config['ckpt_name']),
#                                      map_location=device))
#     return model, voc, test_dataset, config


# def infer(args):
#     if not os.path.exists('samples'):
#         os.mkdir('samples')

#     model, voc, test_dataset, config = load_model_and_dataset(args)
#     import cv2
#     num_samples = 10
#     for i in tqdm(range(num_samples)):
#         dataset_idx = random.randint(0, len(voc))
#         im_tensor, target, fname = voc[dataset_idx]
#         detr_output = model(
#             im_tensor.unsqueeze(0).to(device),
#             score_thresh=config['train_params']['infer_score_threshold'],
#             use_nms=config['train_params']['use_nms_infer']
#         )
#         detr_detections = detr_output['detections']
#         enc_attn_weights = detr_output['enc_attn']
#         dec_attn_weights = detr_output['dec_attn']

#         gt_im = cv2.imread(fname)
#         h, w = gt_im.shape[:2]
#         gt_im_copy = gt_im.copy()
#         # Saving images with ground truth boxes
#         for idx, box in enumerate(target['boxes']):
#             x1, y1, x2, y2 = box.detach().cpu().numpy()
#             x1, y1, x2, y2 = int(w*x1), int(h*y1), int(w*x2), int(h*y2)
#             cv2.rectangle(gt_im, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])
#             cv2.rectangle(gt_im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])
#             text = voc.idx2label[target['labels'][idx].detach().cpu().item()]
#             text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)
#             text_w, text_h = text_size
#             cv2.rectangle(gt_im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)
#             cv2.putText(gt_im, text=voc.idx2label[target['labels'][idx].detach().cpu().item()],
#                         org=(x1 + 5, y1 + 15),
#                         thickness=1,
#                         fontScale=1,
#                         color=[0, 0, 0],
#                         fontFace=cv2.FONT_HERSHEY_PLAIN)
#             cv2.putText(gt_im_copy, text=text,
#                         org=(x1 + 5, y1 + 15),
#                         thickness=1,
#                         fontScale=1,
#                         color=[0, 0, 0],
#                         fontFace=cv2.FONT_HERSHEY_PLAIN)
#         cv2.addWeighted(gt_im_copy, 0.7, gt_im, 0.3, 0, gt_im)
#         cv2.imwrite('samples/output_detr_gt_{}.png'.format(i), gt_im)

#         # Getting predictions from trained model
#         boxes = detr_detections[0]['boxes']
#         labels = detr_detections[0]['labels']
#         scores = detr_detections[0]['scores']
#         im = cv2.imread(fname)
#         im_copy = im.copy()

#         # Saving images with predicted boxes
#         for idx, box in enumerate(boxes):
#             x1, y1, x2, y2 = box.detach().cpu().numpy()
#             x1, y1, x2, y2 = int(w*x1), int(h*y1), int(w*x2), int(h*y2)
#             cv2.rectangle(im, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])
#             cv2.rectangle(im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])
#             text = '{} : {:.2f}'.format(voc.idx2label[labels[idx].detach().cpu().item()],
#                                         scores[idx].detach().cpu().item())
#             text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)
#             text_w, text_h = text_size
#             cv2.rectangle(im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)
#             cv2.putText(im, text=text,
#                         org=(x1 + 5, y1 + 15),
#                         thickness=1,
#                         fontScale=1,
#                         color=[0, 0, 0],
#                         fontFace=cv2.FONT_HERSHEY_PLAIN)
#             cv2.putText(im_copy, text=text,
#                         org=(x1 + 5, y1 + 15),
#                         thickness=1,
#                         fontScale=1,
#                         color=[0, 0, 0],
#                         fontFace=cv2.FONT_HERSHEY_PLAIN)
#         cv2.addWeighted(im_copy, 0.7, im, 0.3, 0, im)
#         cv2.imwrite('samples/output_detr_{}.jpg'.format(i), im)
#     print('Done Detecting...')


# def evaluate_map(args):
#     model, voc, test_dataset, config = load_model_and_dataset(args)

#     gts = []
#     preds = []
#     difficults = []
#     for im_tensor, target, fname in tqdm(test_dataset):
#         im_tensor = im_tensor.float().to(device)
#         target_bboxes = target['boxes'].float()[0].to(device)
#         target_labels = target['labels'].long()[0].to(device)
#         difficult = target['difficult'].long()[0].to(device)
#         detr_output = model(
#             im_tensor,
#             score_thresh=config['train_params']['eval_score_threshold'],
#             use_nms=config['train_params']['use_nms_eval']
#         )
#         detr_detections = detr_output['detections']

#         boxes = detr_detections[0]['boxes']
#         labels = detr_detections[0]['labels']
#         scores = detr_detections[0]['scores']

#         pred_boxes = {}
#         gt_boxes = {}
#         difficult_boxes = {}

#         for label_name in voc.label2idx:
#             pred_boxes[label_name] = []
#             gt_boxes[label_name] = []
#             difficult_boxes[label_name] = []

#         for idx, box in enumerate(boxes):
#             x1, y1, x2, y2 = box.detach().cpu().numpy()
#             label = labels[idx].detach().cpu().item()
#             score = scores[idx].detach().cpu().item()
#             label_name = voc.idx2label[label]
#             pred_boxes[label_name].append([x1, y1, x2, y2, score])
#         for idx, box in enumerate(target_bboxes):
#             x1, y1, x2, y2 = box.detach().cpu().numpy()
#             label = target_labels[idx].detach().cpu().item()
#             label_name = voc.idx2label[label]
#             gt_boxes[label_name].append([x1, y1, x2, y2])
#             difficult_boxes[label_name].append(difficult[idx].detach().cpu().item())

#         gts.append(gt_boxes)
#         preds.append(pred_boxes)
#         difficults.append(difficult_boxes)

#     mean_ap, all_aps = compute_map(preds, gts, method='area', difficult=difficults)
#     print('Class Wise Average Precisions')
#     for idx in range(len(voc.idx2label)):
#         print('AP for class {} = {:.4f}'.format(voc.idx2label[idx],
#                                                 all_aps[voc.idx2label[idx]]))
#     print('Mean Average Precision : {:.4f}'.format(mean_ap))


# if __name__ == '__main__':
#     parser = argparse.ArgumentParser(description='Arguments for detr inference')
#     parser.add_argument('--config', dest='config_path',
#                         default='config/voc.yaml', type=str)
#     parser.add_argument('--evaluate', dest='evaluate',
#                         default=True, type=bool)
#     parser.add_argument('--infer_samples', dest='infer_samples',
#                         default=True, type=bool)
#     args = parser.parse_args()

#     with torch.no_grad():
#         if args.infer_samples:
#             infer(args)
#         else:
#             print('Not Inferring for samples as `infer_samples` argument is False')

#         if args.evaluate:
#             evaluate_map(args)
#         else:
#             print('Not Evaluating as `evaluate` argument is False')

%%writefile tools/infer.py
import torch
import argparse
import os
import yaml
import random
from tqdm import tqdm
from model.detr import DETR
import numpy as np
import cv2
# --- THAY ĐỔI 1: Import Dataset mới ---
from dataset.food67 import Food67Dataset
from torch.utils.data.dataloader import DataLoader

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.backends.mps.is_available():
    device = torch.device('mps')
    print('Using mps')


def get_iou(det, gt):
    det_x1, det_y1, det_x2, det_y2 = det
    gt_x1, gt_y1, gt_x2, gt_y2 = gt

    x_left = max(det_x1, gt_x1)
    y_top = max(det_y1, gt_y1)
    x_right = min(det_x2, gt_x2)
    y_bottom = min(det_y2, gt_y2)

    if x_right < x_left or y_bottom < y_top:
        return 0.0

    area_intersection = (x_right - x_left) * (y_bottom - y_top)
    det_area = (det_x2 - det_x1) * (det_y2 - det_y1)
    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)
    area_union = float(det_area + gt_area - area_intersection + 1E-6)
    iou = area_intersection / area_union
    return iou


def compute_map(det_boxes, gt_boxes, iou_threshold=0.5, method='area', difficult=None):
    gt_labels = {cls_key for im_gt in gt_boxes for cls_key in im_gt.keys()}
    gt_labels = sorted(gt_labels)

    all_aps = {}
    aps = []
    for idx, label in enumerate(gt_labels):
        cls_dets = [
            [im_idx, im_dets_label] for im_idx, im_dets in enumerate(det_boxes)
            if label in im_dets for im_dets_label in im_dets[label]
        ]

        cls_dets = sorted(cls_dets, key=lambda k: -k[1][-1])

        gt_matched = [[False for _ in im_gts[label]] for im_gts in gt_boxes]
        num_gts = sum([len(im_gts[label]) for im_gts in gt_boxes])
        
        # Food67 không có difficult, mặc định là 0
        num_difficults = sum([sum(difficults_label[label]) for difficults_label in difficult])

        tp = [0] * len(cls_dets)
        fp = [0] * len(cls_dets)

        for det_idx, (im_idx, det_pred) in enumerate(cls_dets):
            im_gts = gt_boxes[im_idx][label]
            im_gt_difficults = difficult[im_idx][label]

            max_iou_found = -1
            max_iou_gt_idx = -1

            for gt_box_idx, gt_box in enumerate(im_gts):
                gt_box_iou = get_iou(det_pred[:-1], gt_box)
                if gt_box_iou > max_iou_found:
                    max_iou_found = gt_box_iou
                    max_iou_gt_idx = gt_box_idx
            
            if max_iou_found >= iou_threshold:
                if not gt_matched[im_idx][max_iou_gt_idx]:
                    gt_matched[im_idx][max_iou_gt_idx] = True
                    tp[det_idx] = 1
                else:
                    fp[det_idx] = 1
            else:
                fp[det_idx] = 1

        tp = np.cumsum(tp)
        fp = np.cumsum(fp)

        eps = np.finfo(np.float32).eps
        recalls = tp / np.maximum(num_gts - num_difficults, eps)
        precisions = tp / np.maximum((tp + fp), eps)

        if method == 'area':
            recalls = np.concatenate(([0.0], recalls, [1.0]))
            precisions = np.concatenate(([0.0], precisions, [0.0]))
            for i in range(precisions.size - 1, 0, -1):
                precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])
            i = np.where(recalls[1:] != recalls[:-1])[0]
            ap = np.sum((recalls[i + 1] - recalls[i]) * precisions[i + 1])
        elif method == 'interp':
            ap = 0.0
            for interp_pt in np.arange(0, 1 + 1E-3, 0.1):
                prec_interp_pt = precisions[recalls >= interp_pt]
                prec_interp_pt= prec_interp_pt.max() if prec_interp_pt.size>0.0 else 0.0
                ap += prec_interp_pt
            ap = ap / 11.0
        else:
            raise ValueError('Method can only be area or interp')
        
        if num_gts > 0:
            aps.append(ap)
            all_aps[label] = ap
        else:
            all_aps[label] = np.nan
    
    if len(aps) > 0:
        mean_ap = sum(aps) / len(aps)
    else:
        mean_ap = 0.0
    return mean_ap, all_aps


def load_model_and_dataset(args):
    with open(args.config_path, 'r') as file:
        try:
            config = yaml.safe_load(file)
        except yaml.YAMLError as exc:
            print(exc)
    print(config)

    dataset_config = config['dataset_params']
    model_config = config['model_params']
    train_config = config['train_params']

    # --- THAY ĐỔI 2: Load Food67Dataset ---
    # Dùng split là 'valid' để đánh giá (theo config yaml là 'test_im_sets')
    voc = Food67Dataset(split='valid',
                        im_sets=dataset_config['test_im_sets'],
                        im_size=dataset_config['im_size'])
    
    test_dataset = DataLoader(voc, batch_size=1, shuffle=False)

    model = DETR(
        config=model_config,
        num_classes=dataset_config['num_classes'],
        bg_class_idx=dataset_config['bg_class_idx']
    )
    model.to(device)
    model.eval()

    ckpt_path = os.path.join(train_config['task_name'], train_config['ckpt_name'])
    assert os.path.exists(ckpt_path), f"No checkpoint exists at {ckpt_path}"
    
    # Load model weights
    state_dict = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(state_dict)
    
    return model, voc, test_dataset, config


def infer(args):
    if not os.path.exists('samples'):
        os.mkdir('samples')

    model, voc, test_dataset, config = load_model_and_dataset(args)
    
    num_samples = 10
    print(f"Inferring on {num_samples} random samples...")
    
    for i in tqdm(range(num_samples)):
        # Lấy ngẫu nhiên 1 ảnh
        dataset_idx = random.randint(0, len(voc) - 1)
        im_tensor, target, fname = voc[dataset_idx]
        
        # Chạy Model
        detr_output = model(
            im_tensor.unsqueeze(0).to(device),
            score_thresh=config['train_params']['infer_score_threshold'],
            use_nms=config['train_params']['use_nms_infer']
        )
        detr_detections = detr_output['detections']
        
        # Đọc ảnh gốc để vẽ
        gt_im = cv2.imread(fname)
        if gt_im is None:
            continue
        h, w = gt_im.shape[:2]
        gt_im_copy = gt_im.copy()
        
        # --- VẼ GROUND TRUTH (Màu Xanh Lá) ---
        # target['boxes'] ở dạng Normalized XYXY [0-1]
        # Logic: x_pixel = x_norm * width
        for idx, box in enumerate(target['boxes']):
            x1, y1, x2, y2 = box.detach().cpu().numpy()
            x1, y1, x2, y2 = int(w*x1), int(h*y1), int(w*x2), int(h*y2)
            
            cv2.rectangle(gt_im, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])
            cv2.rectangle(gt_im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])
            
            label_idx = target['labels'][idx].detach().cpu().item()
            text = voc.idx2label[label_idx]
            
            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)
            text_w, text_h = text_size
            cv2.rectangle(gt_im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)
            cv2.putText(gt_im, text=text, org=(x1 + 5, y1 + 15), thickness=1, fontScale=1, color=[0, 0, 0], fontFace=cv2.FONT_HERSHEY_PLAIN)
            cv2.putText(gt_im_copy, text=text, org=(x1 + 5, y1 + 15), thickness=1, fontScale=1, color=[0, 0, 0], fontFace=cv2.FONT_HERSHEY_PLAIN)
            
        cv2.addWeighted(gt_im_copy, 0.7, gt_im, 0.3, 0, gt_im)
        cv2.imwrite('samples/output_detr_gt_{}.png'.format(i), gt_im)

        # --- VẼ PREDICTION (Màu Đỏ) ---
        if len(detr_detections) > 0:
            boxes = detr_detections[0]['boxes'] # Normalized XYXY [0-1]
            labels = detr_detections[0]['labels']
            scores = detr_detections[0]['scores']
            
            im = cv2.imread(fname)
            im_copy = im.copy()

            for idx, box in enumerate(boxes):
                x1, y1, x2, y2 = box.detach().cpu().numpy()
                x1, y1, x2, y2 = int(w*x1), int(h*y1), int(w*x2), int(h*y2)
                
                cv2.rectangle(im, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])
                cv2.rectangle(im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])
                
                label_idx = labels[idx].detach().cpu().item()
                text = '{} : {:.2f}'.format(voc.idx2label[label_idx], scores[idx].detach().cpu().item())
                
                text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)
                text_w, text_h = text_size
                cv2.rectangle(im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)
                cv2.putText(im, text=text, org=(x1 + 5, y1 + 15), thickness=1, fontScale=1, color=[0, 0, 0], fontFace=cv2.FONT_HERSHEY_PLAIN)
                cv2.putText(im_copy, text=text, org=(x1 + 5, y1 + 15), thickness=1, fontScale=1, color=[0, 0, 0], fontFace=cv2.FONT_HERSHEY_PLAIN)
            
            cv2.addWeighted(im_copy, 0.7, im, 0.3, 0, im)
            cv2.imwrite('samples/output_detr_{}.jpg'.format(i), im)
    
    print('Done Detecting... Check samples/ folder')


def evaluate_map(args):
    model, voc, test_dataset, config = load_model_and_dataset(args)

    gts = []
    preds = []
    difficults = []
    
    print("Evaluating mAP on test set...")
    for im_tensor, target, fname in tqdm(test_dataset):
        im_tensor = im_tensor.float().to(device)
        
        # Batch size = 1
        target_bboxes = target['boxes'].float()[0].to(device) # Normalized [0-1]
        target_labels = target['labels'].long()[0].to(device)
        difficult = target['difficult'].long()[0].to(device)
        
        detr_output = model(
            im_tensor,
            score_thresh=config['train_params']['eval_score_threshold'],
            use_nms=config['train_params']['use_nms_eval']
        )
        detr_detections = detr_output['detections']

        if len(detr_detections) > 0:
            boxes = detr_detections[0]['boxes'] # Normalized [0-1]
            labels = detr_detections[0]['labels']
            scores = detr_detections[0]['scores']
        else:
            boxes, labels, scores = [], [], []

        pred_boxes = {}
        gt_boxes = {}
        difficult_boxes = {}

        for label_name in voc.idx2label:
            if label_name == '__background__': continue
            pred_boxes[label_name] = []
            gt_boxes[label_name] = []
            difficult_boxes[label_name] = []

        # Process Predictions
        if len(boxes) > 0:
            for idx, box in enumerate(boxes):
                x1, y1, x2, y2 = box.detach().cpu().numpy()
                label = labels[idx].detach().cpu().item()
                score = scores[idx].detach().cpu().item()
                label_name = voc.idx2label[label]
                # Lưu ý: Cả GT và Pred đều là Normalized [0-1]. 
                # IoU tính trên 0-1 hay Pixel đều ra kết quả như nhau.
                pred_boxes[label_name].append([x1, y1, x2, y2, score])
        
        # Process Ground Truth
        for idx, box in enumerate(target_bboxes):
            x1, y1, x2, y2 = box.detach().cpu().numpy()
            label = target_labels[idx].detach().cpu().item()
            label_name = voc.idx2label[label]
            gt_boxes[label_name].append([x1, y1, x2, y2])
            difficult_boxes[label_name].append(difficult[idx].detach().cpu().item())

        gts.append(gt_boxes)
        preds.append(pred_boxes)
        difficults.append(difficult_boxes)

    mean_ap, all_aps = compute_map(preds, gts, method='area', difficult=difficults)
    
    print('Class Wise Average Precisions')
    for idx in range(len(voc.idx2label)):
        label_name = voc.idx2label[idx]
        if label_name == '__background__': continue
        if label_name in all_aps:
            print('AP for class {} = {:.4f}'.format(label_name, all_aps[label_name]))
            
    print('Mean Average Precision : {:.4f}'.format(mean_ap))


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Arguments for detr inference')
    parser.add_argument('--config', dest='config_path',
                        default='config/food67.yaml', type=str)
    parser.add_argument('--evaluate', dest='evaluate',
                        default=True, type=bool)
    parser.add_argument('--infer_samples', dest='infer_samples',
                        default=True, type=bool)
    args = parser.parse_args()

    with torch.no_grad():
        if args.infer_samples:
            infer(args)
        else:
            print('Not Inferring for samples as `infer_samples` argument is False')

        if args.evaluate:
            evaluate_map(args)
        else:
            print('Not Evaluating as `evaluate` argument is False')
